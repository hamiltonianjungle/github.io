<!DOCTYPE html>
<html>

<head>
    <title>The Hamiltonian Jungle | MCMC</title>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="description" content="A comprehensive list of known Hamiltonian complexity results." />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css">
    <link rel="stylesheet" href="media/style/default.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Lato">
    <link rel="icon" href="media/graphics/favicon.ico"/>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js"></script>
    <script src="media/scripts/theme.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>MathJax = { tex: { inlineMath: [["$", "$"]] } };</script>
    <link rel="stylesheet" type="text/css" href="https://tikzjax.com/v1/fonts.css">
    <script src="https://tikzjax.com/v1/tikzjax.js"></script>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-MMS1QCMRED"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-MMS1QCMRED');
    </script>
</head>

<body>
    <nav class="navbar navbar-expand-lg sticky-top border-bottom">
        <div class="container-fluid">
            <a class="navbar-brand" href="index.html">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor" viewBox="0 0 16 16"
                    class="d-inline-block align-text-top">
                    <path
                        d="M5 2V0H0v5h2v6H0v5h5v-2h6v2h5v-5h-2V5h2V0h-5v2H5zm6 1v2h2v6h-2v2H5v-2H3V5h2V3h6zm1-2h3v3h-3V1zm3 11v3h-3v-3h3zM4 15H1v-3h3v3zM1 4V1h3v3H1z" />
                </svg>
                <span>The Hamiltonian Jungle</span>
            </a>
            <button class="navbar-toggler p-0 border-0 shadow-none" type="button" data-bs-toggle="offcanvas"
                data-bs-target="#sidebar" aria-controls="sidebar" aria-expanded="false" aria-label="Toggle Navigation">
                <button class="navbar-toggler p-0 border-0 shadow-none" type="button" data-bs-toggle="offcanvas"
                    data-bs-target="#sidebar" aria-controls="sidebar" aria-expanded="false"
                    aria-label="Toggle Navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>
        </div>
    </nav>
    <div class="d-flex">
        <div class="sidebar bg-body-tertiary border-end">
            <div class="sidebar-menu offcanvas-lg offcanvas-start sticky-lg-top" id="sidebar" data-bs-scroll="true"
                aria-labelledby="sidebar">
                <div class="sidebar-offcanvas offcanvas-body p-0 bg-body-tertiary">
                    <div class="container p-0">
                        <ul class="nav flex-column">
                            <li><a class="nav-link" href="#" data-bs-toggle="collapse" data-bs-target="#models"
                                    aria-expanded="false">Models</a></li>
                            <ul class="collapse p-0" id="models">
                                <li><a class="nav-link" href="2-local-pauli.html">2&ndash;Local Pauli</a></li>
                                <li><a class="nav-link" href="bose-hubbard.html">Bose&ndash;Hubbard</a></li>
                                <li><a class="nav-link" href="commuting.html">Commuting</a></li>
                                <li><a class="nav-link" href="fermi-hubbard.html">Fermi&ndash;Hubbard</a></li>
                                <li><a class="nav-link" href="general.html">General</a></li>
                                <li><a class="nav-link" href="general-local.html">General Local</a></li>
                                <li><a class="nav-link" href="heisenberg.html">Heisenberg</a></li>
                                <li><a class="nav-link" href="ising.html">Ising</a></li>
                                <li><a class="nav-link" href="qudit.html">Qudit</a></li>
                                <li><a class="nav-link" href="stoquastic.html">Stoquastic</a></li>
                            </ul>
                            <li><a class="nav-link" href="#" data-bs-toggle="collapse" data-bs-target="#techniques"
                              aria-expanded="true">Techniques</a></li>
                            <ul class="collapse show p-0" id="techniques">
                              <li><a class="nav-link" href="cluster-expansions.html">Cluster Expansions</a></li>
                              <li><a class="nav-link" href="cstar-algebras.html"><span class="cm-sans-serif">C</span><sup>&#42;</sup>&ndash;Algebras</a></li>
                              <li><a class="nav-link" href="gadgets.html">Gadgets</a></li>
                              <li><a class="nav-link" href="mcmc.html" aria-current="true">MCMC</a></li>
                          </ul>
                            <li><a class="nav-link" href="supplementary.html">Supplementary</a></li>
                            <li><a class="nav-link" href="glossary.html">Glossary</a></li>
                            <li><a class="nav-link" href="references.html">References</a></li>
                            <li><a class="nav-link" href="open-problems.html">Open Problems</a></li>
                        </ul>
                        <hr class="m-0 border-top bs-border-color opacity-100" />
                        <ul class="nav flex-column">
                            <li><a class="nav-link" href="about.html">About</a></li>
                            <li><a class="nav-link" href="contact.html">Contact</a></li>
                        </ul>
                        <hr class="m-0 border-top bs-border-color opacity-100" />
                        <ul class="nav flex-column">
                            <li><a class="nav-link" href="#" data-bs-toggle="collapse" data-bs-target="#themes"
                                    aria-expanded="false">Theme</a></li>
                            <ul class="collapse p-0" id="themes">
                                <li><a class="nav-link" href="#" data-bs-theme-value="auto">Auto</a></li>
                                <li><a class="nav-link" href="#" data-bs-theme-value="light">Light</a></li>
                                <li><a class="nav-link" href="#" data-bs-theme-value="dark">Dark</a></li>
                            </ul>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <main class="flex-grow w-100 px-4 pb-4">
            <div class="d-flex pt-3 pb-2 mb-3 border-bottom">
                <h2>Markov Chain Monte Carlo</h2>
            </div>
            <p>
                Markov chain Monte Carlo (MCMC) can be used as an algorithm for calculating the partition function of a Hamiltonian. 
                The obvious way to breakdown this concept is to describe both Monte Carlo and Markov chains. 

                The rough idea is to generate a Markov chain that has a stationary distribution $\pi$ that is the distribution of interest. 
                By sampling the stationary distribution using the Monte Carlo method we can obtain an approximation of the distribution of interest. 
                Each element of the stationary distribution is of the form 
                $$
                    \pi(x) = \frac{w(x)}{\displaystyle\sum_{x\in \Omega} w(x)}.
                $$
                Notice that the denominator is the partition function of the system. 
                Therefore, framing the problem in terms of statistical mechanics we define 
                $$
                    w(x) = e^{-\beta H(x)}.
                $$
                We are interested in approximating the partition function. 
                By designing an ergodic Markov chain with the state space $\Omega$ and the stationary distribution $\pi$ we can sample from the stationary distribution and estimate the partition function. 
                The big problem with this method comes from knowing the mixing time of the Markov chain. 
                It is known via the convergence to equilibrium theorem of Markov chains that any initial distribution will approach the stationary distribution as time tends to infinity. 
                To make the algorithm efficient we require the mixing time to be small. 
            </p>
            <hr class="border-top bs-border-color opacity-100" />
            <h3>Monte Carlo</h3>
            <p>
                Monte Carlo methods are a class of computational algorithms that rely on repeated random sampling to obtain numerical results. 
                Consider some quantity that is defined over a domain $D$, instead of evaluating a quantity directly we can instead sample from the domain according to a probability distribution. 
                Such a probability distribution might be chosen to be the uniform distribution or more catered to the specific problem. 
                For example, it may be appropriate to have the distribution be defined such that certain points in the domain are more likely than others, reflecting an underlying property of the problem. 
                Using the sampled points we can then estimate the quantity of interest. 
                By the law of large numbers the quantity estimated will tend to the true value as the number of samples tends to infinity.
            </p>
            <hr class="border-top bs-border-color opacity-100" />
            <h3>Markov Chains</h3>
            <p>
                A Markov chain is a stochastic process that satisfies the Markov property. 
                The Markov property states that the probability of the next state of the system depends only on the current state and not on the sequence of events that preceded it. 
                A Markov chain is a sequence of random variables $X_1, X_2, \dots$ with the Markov property. 
                The state space of a Markov chain is the set of all possible values that the random variables can take. 
                The transition probabilities of a Markov chain are the probabilities of moving from one state to another. 
                The transition probabilities are usually represented by a matrix called the transition matrix. 
                The transition matrix is a square matrix where the $(i,j)$-th entry is the probability of moving from state $i$ to state $j$ in one step. 
                The transition matrix is usually denoted by $P$. 
                Transition matrices for Markov chains exhibit the stochastic property, i.e. the sum of the entries in each row is equal to $1$,
                $$
                    \sum_{j} P_{ij} = 1.
                $$
                Important properties of Markov chains include the stationary distribution and the convergence to the stationary distribution. 
                The stationary distribution of a Markov chain is a probability distribution that remains unchanged by the transition matrix. 
                The stationary distribution is often denoted by $\pi$ and satisfies the equation (Left and right multiplication variants of the equation exist. They describe the same thing).
                $$
                    \pi = \pi P.
                $$
                The stationary distribution is the distribution that the Markov chain converges to as the number of steps tends to infinity. 
                The convergence to the stationary distribution is a property of the Markov chain and is often characterised by the mixing time. 
                The mixing time is the number of steps required for the Markov chain to converge to the stationary distribution. 
                We denote the mixing time by $\tau$ and it is often the case that $\tau$ is a function of the size of the state space of the Markov chain. 
                A formal definition of the mixing time is given by
                $$
                    \tau(\epsilon) = \min\{t : ||\pi P^t - \pi||_{\text{TV}} \leq \epsilon\},
                $$
                where $||\cdot||_{\text{TV}}$ is the total variation distance. 
            </p>
            <div style="padding-left: 40px; padding-right: 40px; padding-top: 20px; padding-bottom: 20px;">
                <b>Theorem (Convergence to equilibrium) </b>
                Let $P$ be an irreducible and aperiodic transition matrix on a finite state space with stationary distribution $\pi$. 
                Then for any initial distribution $\mu$ the distribution of the Markov chain at time $t$ converges to the stationary distribution as $t\to\infty$,
                    $$
                        ||\mu P^t - \pi||_{\text{TV}} \to 0 \quad \text{as} \quad t\to\infty.
                    $$
            </div>
            <p>
                Notice that one the number of steps is equivalent to the mixing time, all subsequent steps keep the Markov chain at the stationary distribution.
                Therefore, iif there was some way to give the Markov chain a kickstart to the stationary distribution then the mixing time could be reduced.
            </p>
            <hr class="border-top bs-border-color opacity-100" />
            <h3>Glauber Dynamics</h3>
            <p>
                Glauber dynamics <a href="references.html#Gla63">[Gla63]</a> is a type of MCMC that was originally used to measure the statistics of the Ising model. 
                Following the original ides of Glauber, we too consider the MCMC for the Ising model. 
                We introduced the Ising model in the Statistical Mechanics section but recall some details here. 
                We consider the ferromagnetic variant of the Ising model on a graph $G=(V,E)$ with vertex set $V$ and edge set $E$. 
                The Ising model is defined by the Hamiltonian
                $$
                    H(\sigma) = \sum_{(u,v)\in E(G)} J(\sigma_u,\sigma_v).
                $$
                The Gibbs distribution of the Ising model is given by
                $$
                    P(\sigma) = \frac{e^{-\beta H(\sigma)}}{\mathcal{Z}}.
                $$
                We denote a single vertex spin as $\sigma_v$ and $N(\sigma_v)$ as the neighbourhood of spins that are adjacent to $\sigma_v$. 
                Define the ''neighbourhood Hamiltonian'' of a vertex $v$ as 
                $$
                    S_{\sigma_v}(\sigma) = \sum_{u : (u,v)\in E} J(\sigma_u,\sigma_v).
                $$

                Glauber dynamics operating as a Markov chain mimics the systems evolution towards thermal equilibrium at the given temperature. 
                The transition probabilities of the Markov chain are related to the Maxwell-Boltzmann distribution of the chosen vertex and its neighbours. 
                Running the dynamics for sufficient time allows the system to tend to equilibrium where in which the spin distribution is stabelised. 
                Then at this equilibrium, the probability of a particular spin configuration is given by the Gibbs distribution. 
                Sampling states from this system after equilibrium is reached means we then sample from the Gibbs distribution.
            </p>
            <p>
                Given some underlying graph $G = (V,E)$ and inverse temperature $\beta$ and an initial configuration of spins $\sigma$, 
                the Glauber dynamics describe a reversible Markov chain that obtains the next configuration $\sigma'$ by flipping a single spin in $\sigma$. 
                The algorithm follows as <a href="references.html#Gla63">[Gla63]</a>,<a href="references.html#HS07">[HS07]</a>,<a href="references.html#MS13">[MS13]</a>,<a href="references.html#CHK21">[CHK21]</a>:
                <ol>
                    <li>Pick a vertex $v$ uniformly at random.</li>
                    <li>Let $\sigma'(u) = \sigma(u)$ for all $u\neq v$.</li>
                    <li>Define $\sigma'(v) = +1$ with probability $\mathbb{P}(\sigma_v = +1 | N(\sigma_v))$.</li>
                </ol>
                The dynamics here are often referred to as <i>heat-bath dynamics</i> and are a special case of the Metropolis-Hastings algorithm. 
                We define the transition probability of the Glauber dynamics as
                $$
                    \mathbb{P}(\sigma_v = +1 | N(\sigma_v)) = \frac{\exp(-\beta S_{+1}(\sigma))}{\exp(-\beta S_{+1}(\sigma)) + \exp(-\beta S_{-1}(\sigma))},
                $$
                which are local in that they only depend on the neighbourhood of the chosen vertex. 
                Additionally, they are reversible with respect to a distribution and hence satisfy the Metropolis detailed balance condition 
                $$
                    \pi(\sigma) \mathbb{P}(\sigma \to \sigma') = \pi(\sigma') \mathbb{P}(\sigma' \to \sigma).
                $$
                It is therefore very natural to see why the transition probabilities of the Glauber dynamics are related to the Gibbs distribution of the neighbourhood of the chosen vertex.
            </p>
            <p>
                A neat way to simplify the transition probability is to define $J(u,v) = 1 - \delta_{u,v}$ then notice that $J(u,v) = 1$ if and only if $\sigma_u = -\sigma_v$;
                Think of this as an energy <i>penalty</i> for the spins being anti-aligned. 
                Denoting the number of neighbours of $v$ that are $+1$ as $p$ and the number of neighbours of $v$ that are $-1$ as $m$ we can write
                $$
                    S_{+1}(\sigma) = m, \quad S_{-1}(\sigma) = p.
                $$
                Let $\lambda = e^{-\beta}$ then the transition probability becomes
                $$
                    \mathbb{P}(\sigma_v = +1 | N(\sigma_v)) = \frac{\lambda^m}{\lambda^m + \lambda^p}.
                $$
                The quantity of interest here is the mixing time $\tau$ which is the number of steps needed to get close to the Gibbs distribution. 
                It is common to define an error bound $\epsilon$ and then find the number of steps needed to get within $\epsilon$ of the stationary distribution. 
                Typically it is chosen that $\epsilon = 1/2e$ then for times $\eta \tau$ then total variation distance is less than $\epsilon^{-\eta}$ <a href="references.html#HS07">[HS07]</a>,<a href="references.html#MS13">[MS13]</a>,<a href="references.html#AS17">[AS17]</a>. 
                While the analysis in bounding the mixing time is fairly involved we are still able to quote some results regarding Glauber dynamics.
            </p>
            <p>
                It was shown by Hayes and Sinclair that the mixing time for discrete Glauber dynamics on the ferromagnetic Ising model is lower bounded by $\Omega(n\log n / (\Delta \log^2 \Delta))$ when the underlying graph $G$ is of bounded degree $\Delta \geq 2$. 
                This bound was improved by Ding and Peres for an graph with $n$ vertices giving a lower bound of $(1/4 + o(1))n\log n$ <a href="references.html#DP11">[DP11]</a> (This bound is specifically for the ferromagnetic Ising model). 
            </p>
        </main>
    </div>
</body>

</html>
